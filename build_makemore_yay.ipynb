{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMfUUiMkAiHOd5Uks3Idk96",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cwscx/makmore/blob/main/build_makemore_yay.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RGbg91TB5bFj"
      },
      "outputs": [],
      "source": [
        "# from google.colab import files\n",
        "# uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = open('names.txt', 'r').read().splitlines()"
      ],
      "metadata": {
        "id": "DoG6JmfVhVsP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words[:10]"
      ],
      "metadata": {
        "id": "QyVZwNIxhdpL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(words)"
      ],
      "metadata": {
        "id": "L236mAfuhmpM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "min(len(w) for w in words)"
      ],
      "metadata": {
        "id": "t_ZSAS0AhpGG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max(len(w) for w in words)"
      ],
      "metadata": {
        "id": "Yz9uM4r4hw2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "b = {}\n",
        "for w in words:\n",
        "  chs = ['<S>'] + list(w) + ['<W>']\n",
        "  for ch1, ch2 in zip(chs, chs[1:]):\n",
        "    bigram = (ch1, ch2)\n",
        "    b[bigram] = b.get(bigram, 0) + 1"
      ],
      "metadata": {
        "id": "TzCG9u1tkVrm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sorted(b.items(), key = lambda kv: kv[1], reverse=True)"
      ],
      "metadata": {
        "id": "7AN5TqOokRPA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "q7ie1_Iklk1N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stoi = {s:i for i, s in enumerate(list('.abcdefghijklmnopqrstuvwxyz'))}\n",
        "itos = {i:s for s,i in stoi.items()}"
      ],
      "metadata": {
        "id": "T973Z3zapipz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N = torch.zeros((27, 27), dtype=torch.int32)\n",
        "for w in words:\n",
        "  chs = ['.'] + list(w) + ['.']\n",
        "  for ch1, ch2 in zip(chs, chs[1:]):\n",
        "    index1 = stoi.get(ch1, 0)\n",
        "    index2 = stoi.get(ch2, 0)\n",
        "    N[index1, index2] += 1\n"
      ],
      "metadata": {
        "id": "vPU7hCv4lwM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "plt.figure(figsize=(16,16))\n",
        "plt.imshow(N, cmap=\"Blues\")\n",
        "for i in range(27):\n",
        "  for j in range(27):\n",
        "    chstr = itos[i] + itos[j]\n",
        "    plt.text(j, i, chstr, ha=\"center\", va=\"bottom\", color=\"gray\")\n",
        "    plt.text(j, i, N[i,j].item(), ha=\"center\", va=\"top\", color=\"gray\")\n",
        "plt.axis('off')"
      ],
      "metadata": {
        "id": "PY_MlTeDm63x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N[0, :]"
      ],
      "metadata": {
        "id": "WmTkTAz3sYnS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p = N[0].float()\n",
        "p = p / p.sum()\n",
        "p"
      ],
      "metadata": {
        "id": "1_dV0kaktAS1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "g = torch.Generator().manual_seed(2147483647)\n",
        "index = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
        "print(index)\n",
        "print(itos[index])"
      ],
      "metadata": {
        "id": "7hHeTAXzqoHq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "g = torch.Generator().manual_seed(2147483647)\n",
        "p = torch.rand(3, generator=g)\n",
        "print(p)\n",
        "p = p / p.sum()\n",
        "print(p)"
      ],
      "metadata": {
        "id": "wYqMTkjstMCK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t = torch.multinomial(p,  num_samples=20, replacement=True, generator=g)"
      ],
      "metadata": {
        "id": "B0GpQiISpsJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "P = (N + 1).float() # + 1 is model smoothing. Make model more smooth (less peaked)\n",
        "\n",
        "# braodcast.\n",
        "# P [27,27]. P.sum [27,1]. It scales P.sum to [27,27] by copying the value, and\n",
        "# apply semantic operation 1 by 1.\n",
        "#\n",
        "# sum(dim=1) could be understand as P[c][i] where c is constant, sum all i.\n",
        "P /=P.sum(dim=1, keepdim=True)  # Use /= in place operation to avoid creating new object P\n"
      ],
      "metadata": {
        "id": "zcHdkBdCz1ld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "g = torch.Generator().manual_seed(2147483647)\n",
        "\n",
        "for i in range(5):\n",
        "  out = []\n",
        "  index = 0\n",
        "  while True:\n",
        "    p = P[index]\n",
        "    # xenc = F.one_hot(torch.tensor([index]), num_classes=27).float()\n",
        "    # logits = xenc @ W\n",
        "    # counts = logits.exp()\n",
        "    # p = counts / counts.sum(dim=1, keepdim=True)\n",
        "    index = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
        "    out.append(itos[index])\n",
        "    if index == 0:\n",
        "      break\n",
        "\n",
        "  print(''.join(out))"
      ],
      "metadata": {
        "id": "tAZlJ39ZtwLF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.Tensor([[1,2,3],[4,5,6]])\n",
        "a.sum(0, keepdim=True)"
      ],
      "metadata": {
        "id": "GS6N9kZSDLSl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "P[:,0].sum() # sum all first element per row."
      ],
      "metadata": {
        "id": "NCbd9L5eDOPo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# likelihood is the product of possibility.\n",
        "# but unlike P which was calculated by us from the dataset. The actual P will\n",
        "# be calculated by neural network.\n",
        "negative_log_likelihood = 0 # lowest possible value is 0\n",
        "n = 0\n",
        "for w in [\"andrejq\"]:\n",
        "  chs = ['.'] + list(w) + ['.']\n",
        "  for ch1, ch2 in zip(chs, chs[1:]):\n",
        "    index1 = stoi.get(ch1, 0)\n",
        "    index2 = stoi.get(ch2, 0)\n",
        "    prob=P[index1][index2]\n",
        "    logprob = torch.log(prob)\n",
        "    negative_log_likelihood -= logprob\n",
        "    n += 1\n",
        "    print(f'{ch1}-{ch2}: {prob:.4f} {logprob:.4f}')\n",
        "\n",
        "print(f'{negative_log_likelihood=}')\n",
        "print(f'{negative_log_likelihood / n}')"
      ],
      "metadata": {
        "id": "PnRyBNvCFuXy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # creating the training set of the bigrams\n",
        "\n",
        "xs = []\n",
        "ys = []\n",
        "for w in words:\n",
        "  chs = ['.'] + list(w) + ['.']\n",
        "  for ch1, ch2 in zip(chs, chs[1:]):\n",
        "    index1 = stoi.get(ch1, 0)\n",
        "    index2 = stoi.get(ch2, 0)\n",
        "    xs.append(index1)\n",
        "    ys.append(index2)\n",
        "\n",
        "# prefer tensor over Tensor. Because tensor could defer dtype automatically.\n",
        "xs = torch.tensor(xs)\n",
        "ys = torch.tensor(ys)"
      ],
      "metadata": {
        "id": "Owm-XPaAX58M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Each example is just an integer. And an int cannot be plugged in neural network.\n",
        "# to do so, use encoding to encode the integer to vector.\n",
        "# one hot encoding.\n",
        "from torch.nn import functional as F\n",
        "\n",
        "x_encoded = F.one_hot(xs, num_classes=27).float()\n",
        "y_encoded = F.one_hot(ys, num_classes=27).float()"
      ],
      "metadata": {
        "id": "nrUVWaN7ZoTG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(x_encoded)"
      ],
      "metadata": {
        "id": "wDdeV82hcRoG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_encoded.dtype"
      ],
      "metadata": {
        "id": "Hx-5KiQXcsug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "g = torch.Generator().manual_seed(2147483647)\n",
        "W = torch.randn((27, 27), generator=g, requires_grad=True)"
      ],
      "metadata": {
        "id": "0BI5CUcouMb3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. we use weight to convert data (count) into logits.\n",
        "logits = x_encoded @ W\n",
        "\n",
        "# ------softmax start------\n",
        "# 2. we run exp over logits to convert it into sth like counts (positive)\n",
        "counts = logits.exp()\n",
        "\n",
        "# 3. we then normalize it to get probs, ready for backward progpagation.\n",
        "# because all the operations @, exp, normalize are differentiable.\n",
        "probs = counts / counts.sum(dim=1, keepdim=True)\n",
        "# ------softmax end--------\n",
        "\n",
        "print(probs)"
      ],
      "metadata": {
        "id": "3ArE-tNasxsr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "probs.shape"
      ],
      "metadata": {
        "id": "RQkGdn_ms4os"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlls = torch.zeros(5)\n",
        "for i in range(5):\n",
        "  x = xs[i].item()\n",
        "  y = ys[i].item()\n",
        "\n",
        "  print('-------------------------')\n",
        "  print(f'bigram example {i + 1}: {itos[x]}{itos[y]} (indexes) {x}, {y}')\n",
        "  print('input to neural net:', x)\n",
        "  print('output probabilities from neural net:', probs[i])\n",
        "  print('label:', y)\n",
        "  p = probs[i, y]\n",
        "  print('probabilby assigned by net to the label:', p.item())\n",
        "  logp = torch.log(p)\n",
        "  print('log probablity assigned by net to the label:', logp.item())\n",
        "  nll = -logp\n",
        "  print('negative log likelihood:', nll.item())\n",
        "  nlls[i] = nll\n",
        "\n",
        "print(\"========\")\n",
        "print('average negative log likelihood', nlls.mean().item())"
      ],
      "metadata": {
        "id": "qRm6BEo8vSd8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " probs[0, 5], probs[1,13], probs[2, 13], probs[3, 1], probs[4, 0]"
      ],
      "metadata": {
        "id": "L-h2AC-kxu0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for k in range(100):\n",
        "  # Forward pass\n",
        "  x_encoded = F.one_hot(xs, num_classes=27).float() # num_of_words x 27\n",
        "  logits = x_encoded @ W # num_of_words x 27\n",
        "  count = logits.exp() # num_of_words x 27\n",
        "  softmax_probs = count / count.sum(dim=1, keepdim=True) # num_of_words x 27\n",
        "  loss = -softmax_probs[torch.arange(len(xs)), ys].log().mean()\n",
        "  # loss = -softmax_probs[torch.arange(len(xs)), ys].log().mean() + 0.01 * (W**2).mean() # make the loss score more smooth\n",
        "\n",
        "  print(loss.item())\n",
        "\n",
        "  # Backward pass\n",
        "  W.grad = torch.zeros((27, 27), dtype=torch.float)\n",
        "  loss.backward()\n",
        "  W.data += -50 * W.grad"
      ],
      "metadata": {
        "id": "RwBg1M9DzoGV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}