{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMAIPYXeZZxhfy5Iy1U2dxP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cwscx/makmore/blob/main/makemore_becoming_a_backprop_ninjia.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "32XoMPIxQFcT"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import files\n",
        "# uploaded = files.upload()"
      ],
      "metadata": {
        "id": "0Dun3iOlcUHM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = open('names.txt', 'r').read().splitlines()"
      ],
      "metadata": {
        "id": "-nPNyW1bcWYx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(words)"
      ],
      "metadata": {
        "id": "A1HuORnUceIN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stoi = {c:i for i, c in enumerate('.abcdefghijklmnopqrstuvwxyz')}\n",
        "itos = {i:c for c, i in stoi.items()}"
      ],
      "metadata": {
        "id": "u9DRdd6FcfJv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build inputs.\n",
        "block_size = 3\n",
        "\n",
        "def build_dataset(words: list[str]):\n",
        "  x: list[list[int]] = []\n",
        "  y: list[int] = []\n",
        "  for w in words:\n",
        "    inputs = [0] * block_size\n",
        "    output = None\n",
        "    for c in w + '.':\n",
        "      c_index = stoi[c]\n",
        "      x.append(inputs)\n",
        "      y.append(c_index)\n",
        "      inpupts = inputs[1:] + [c_index]\n",
        "\n",
        "  return torch.tensor(x), torch.tensor(y)\n",
        "\n",
        "import random\n",
        "random.seed(42)\n",
        "random.shuffle(words)\n",
        "\n",
        "n1 = int(len(words) * 0.8)\n",
        "n2 = int(len(words) * 0.9)\n",
        "\n",
        "training_x, training_y = build_dataset(words[:n1])\n",
        "dev_x, dev_y = build_dataset(words[n1:n2])\n",
        "test_x, test_y = build_dataset(words[n2:])"
      ],
      "metadata": {
        "id": "K3JhGHymcrhF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cmp(s, dt, t):\n",
        "  ex = torch.all(dt == t.grad).item()\n",
        "  app = torch.allclose(dt, t.grad)\n",
        "  maxdiff = (dt - t.grad).abs().max().item()\n",
        "  print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')"
      ],
      "metadata": {
        "id": "5fXTVCUWNHJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_embed = 10\n",
        "n_hidden = 64\n",
        "vocab_size = len(itos)\n",
        "\n",
        "g = torch.Generator().manual_seed(2147483647)\n",
        "C = torch.randn((vocab_size, 10), generator=g)\n",
        "W1 = torch.randn((block_size * n_embed, n_hidden), generator=g) * (5/3) / ((block_size * n_embed)**0.5)\n",
        "b1 = torch.randn((n_hidden), generator=g) * 0.1\n",
        "W2 = torch.randn((n_hidden, vocab_size), generator=g) * 0.1\n",
        "b2 = torch.randn((vocab_size), generator=g) * 0\n",
        "\n",
        "bngain = torch.ones((1, n_hidden)) * 0.1 + 1.0\n",
        "bnbias = torch.zeros((1, n_hidden)) * 0.1\n",
        "\n",
        "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
        "\n",
        "for p in parameters:\n",
        "  p.requires_grad = True"
      ],
      "metadata": {
        "id": "-TKofsVAeJVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "n = batch_size\n",
        "\n",
        "ix = torch.randint(0, high=training_x.shape[0], size=(batch_size,), generator=g)\n",
        "\n",
        "Xb = training_x[ix]\n",
        "Yb = training_y[ix]"
      ],
      "metadata": {
        "id": "hwDg_Z9RCoIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "emb = C[Xb]\n",
        "embcat = emb.view(emb.shape[0], -1)\n",
        "\n",
        "# Linear layer 1\n",
        "hprebn = embcat @ W1 + b1 # pre activation\n",
        "\n",
        "# BatchNorm layer\n",
        "bnmeani = 1 / n * hprebn.sum(0, keepdim=True)\n",
        "bndiff = hprebn - bnmeani\n",
        "bndiff2 = bndiff**2\n",
        "bnvar = 1 / (n - 1) * bndiff2.sum(0, keepdim=True)\n",
        "bnvar_inv = (bnvar + 1e-5) ** -0.5\n",
        "bnraw = bndiff * bnvar_inv\n",
        "hpreact = bngain * bnraw + bnbias\n",
        "\n",
        "# Non-linearity\n",
        "h = torch.tanh(hpreact)\n",
        "\n",
        "# Linear layer 2\n",
        "logits = h @ W2 + b2\n",
        "\n",
        "# Cross entropy\n",
        "logit_maxes = logits.max(1, keepdim=True).values\n",
        "norm_logits = logits - logit_maxes\n",
        "counts = norm_logits.exp()\n",
        "counts_sum = counts.sum(1, keepdim=True)\n",
        "counts_sum_inv = counts_sum ** -1\n",
        "probs = counts * counts_sum_inv\n",
        "logprobs = probs.log()\n",
        "# it should be\n",
        "# log(y_target) - log(y_predict)\n",
        "# log(1) - log(y_predict)\n",
        "# 0 - log(y_predict)\n",
        "# -log(y_predict)\n",
        "loss = -logprobs[range(n), Yb].mean()\n",
        "\n",
        "# bnmean_running = 0.999 * bnmean_running + 0.001 * batch_norm_mean\n",
        "# bnstd_running = 0.999 * bnstd_running + 0.001 * batch_norm_std\n",
        "\n",
        "for p in parameters:\n",
        "  p.grad = None\n",
        "\n",
        "for t in [logprobs, probs, counts, counts_sum, counts_sum_inv,\n",
        "          norm_logits, logit_maxes, logits, h, hpreact, bnraw,\n",
        "          bnvar_inv, bnvar, bndiff2, bndiff, hprebn, bnmeani,\n",
        "          embcat, emb]:\n",
        "  t.retain_grad()\n",
        "\n",
        "loss.backward()"
      ],
      "metadata": {
        "id": "3HtnI0B_eKws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(emb.shape)\n",
        "print(C.shape)\n",
        "print(Xb.shape)"
      ],
      "metadata": {
        "id": "3GynlewDRrXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Excersise 1: backprop through the whole thing manually,\n",
        "# backpropagating throug exactly all of the variables as\n",
        "# they are defined in the forward pass above, one by one.\n",
        "\n",
        "# --------Code start-----------\n",
        "import math\n",
        "\n",
        "dlogprobs = torch.zeros(logprobs.shape)\n",
        "dlogprobs[range(n), Yb] = -1.0 / n\n",
        "\n",
        "dprobs = 1 / probs * dlogprobs\n",
        "\n",
        "# shape is different.\n",
        "# c = a * b\n",
        "# a[3x3] * b[3x1]\n",
        "# a11*b1 a12*b1 a13*b1\n",
        "# a21*b2 a22*b2 a23*b2\n",
        "# a31*b3 a32*b3 a33*b3\n",
        "dcounts_sum_inv = (counts * dprobs).sum(dim=1, keepdim=True)\n",
        "dcounts_sum = - 1 / counts_sum**2 * dcounts_sum_inv\n",
        "\n",
        "# count_sum_div also depend on counts. Need two calculation.\n",
        "# b = sum(a)\n",
        "# a11 a12 a13 --> b1\n",
        "# a21 a22 a23 --> b2\n",
        "# a31 a32 a33 --> b3\n",
        "# db  / da = 1\n",
        "dcounts = counts_sum_inv * dprobs + dcounts_sum\n",
        "\n",
        "dnorm_logits = norm_logits.exp() * dcounts\n",
        "\n",
        "# c = b - a\n",
        "# b(32x27) a(32x1)\n",
        "# b11-a1 b12-a1 b13-a1 ... b127-a1\n",
        "dlogit_maxes = (-dnorm_logits).sum(dim=1, keepdim=True)\n",
        "\n",
        "# max\n",
        "# b = max(a1, a2, a3, ..., a27)\n",
        "# only the one that is taken has grad 1, others are 0.\n",
        "dlogits = dnorm_logits.clone() + F.one_hot(logits.max(1).indices, num_classes=logits.shape[1]) * dlogit_maxes\n",
        "\n",
        "dh = dlogits @ W2.T\n",
        "dW2 = torch.transpose(h, 0, 1) @ dlogits\n",
        "db2 = dlogits.sum(0, keepdim=True)\n",
        "\n",
        "# y = tanh(x)\n",
        "# dy / dx = 1 - y**2\n",
        "dhpreact = dh * (1 - h**2)\n",
        "\n",
        "dbngain = (dhpreact * bnraw).sum(dim=0, keepdim=True)\n",
        "dbnbias = dhpreact.sum(dim=0, keepdim=True)\n",
        "dbnraw = dhpreact * bngain\n",
        "\n",
        "dbnvar_inv = (dbnraw * bndiff).sum(dim=0, keepdim=True)\n",
        "dbnvar = dbnvar_inv * -0.5 * (bnvar + 1e-5)**-1.5\n",
        "dbndiff2 = (1.0 / (n-1)) * torch.ones_like(bndiff2) * dbnvar\n",
        "\n",
        "dbndiff = 2.0 * bndiff * dbndiff2 + bnvar_inv * dbnraw\n",
        "\n",
        "dbnmeani = (-dbndiff).sum(dim=0, keepdim=True)\n",
        "dhprebn = dbndiff + torch.ones_like(hprebn) * (dbnmeani / n)\n",
        "\n",
        "dembcat = dhprebn @ W1.T\n",
        "dW1 = embcat.T @ dhprebn\n",
        "db1 = dhprebn.sum(0)\n",
        "\n",
        "demb = dembcat.view(emb.shape[0], emb.shape[1], emb.shape[2])\n",
        "dC = torch.zeros_like(C)\n",
        "for k in range(Xb.shape[0]):\n",
        "  for j in range(Xb.shape[1]):\n",
        "    ix = Xb[k ,j]\n",
        "    dC[ix] += demb[k, j]\n",
        "# --------Code end-------------\n",
        "\n",
        "\n",
        "# --------Test start-----------\n",
        "cmp('logprobs', dlogprobs, logprobs)\n",
        "cmp('probs', dprobs, probs)\n",
        "cmp('counts_sum_inv', dcounts_sum_inv, counts_sum_inv)\n",
        "cmp('counts_sum', dcounts_sum, counts_sum)\n",
        "cmp('counts', dcounts, counts)\n",
        "cmp('norm_logits', dnorm_logits, norm_logits)\n",
        "cmp('logit_maxes', dlogit_maxes, logit_maxes)\n",
        "cmp('logits', dlogits, logits)\n",
        "cmp('h', dh, h)\n",
        "cmp('W2', dW2, W2)\n",
        "cmp('b2', db2, b2)\n",
        "cmp('hpreact', dhpreact, hpreact)\n",
        "cmp('bngain', dbngain, bngain)\n",
        "cmp('bnbias', dbnbias, bnbias)\n",
        "cmp('bnraw', dbnraw, bnraw)\n",
        "cmp('bnvar_inv', dbnvar_inv, bnvar_inv)\n",
        "cmp('bnvar', dbnvar, bnvar)\n",
        "cmp('bndiff2', dbndiff2, bndiff2)\n",
        "cmp('bndiff', dbndiff, bndiff)\n",
        "cmp('bnmeani', dbnmeani, bnmeani)\n",
        "cmp('hprebn', dhprebn, hprebn)\n",
        "cmp('embcat', dembcat, embcat)\n",
        "cmp('W1', dW1, W1)\n",
        "cmp('b1', db1, b1)\n",
        "cmp('emb', demb, emb)\n",
        "cmp('C', dC, C)\n",
        "# --------Test end-------------"
      ],
      "metadata": {
        "id": "1wU3c__bN0Pl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 2: backprop through cross entropy but all in one go\n",
        "# to complete this challenge look at the mathmatical expression\n",
        "# of the loss, take the derivative, simplify the expression, and\n",
        "# just write it out.\n",
        "\n",
        "# forward pass\n",
        "\n",
        "# before:\n",
        "# logit_maxes = logits.max(1, keepdim=True).values\n",
        "# norm_logits = logits - logit_maxes\n",
        "# counts = norm_logits.exp()\n",
        "# counts_sum = counts.sum(1, keepdim=True)\n",
        "# counts_sum_inv = counts_sum ** -1\n",
        "# probs = counts * counts_sum_inv\n",
        "# logprobs = probs.log()\n",
        "# loss = -logprobs[range(n), Yb].mean()\n",
        "\n",
        "# now\n",
        "loss_fast = F.cross_entropy(logits, Yb)\n",
        "print(loss_fast.item(), 'diff:', (loss_fast - loss).item())"
      ],
      "metadata": {
        "id": "wf1Ank7lFH6X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# backward pass\n",
        "\n",
        "#------Code start--------\n",
        "# loss = -log(p_y)\n",
        "#      = -log(e^l_i / sum(e^l))\n",
        "# dlogits = dloss / dl_i\n",
        "#\n",
        "# dloss / dl_i = p_i if not_match else 1 - p_i\n",
        "dlogits = F.softmax(logits, dim=1)\n",
        "dlogits[range(n), Yb] -= 1\n",
        "dlogits /= n\n",
        "#------Code end----------\n",
        "\n",
        "cmp('logits', dlogits, logits) # 6e-9"
      ],
      "metadata": {
        "id": "nbZFKoJoF7iL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(3,3))\n",
        "plt.imshow(dlogits.detach(), cmap=\"gray\")"
      ],
      "metadata": {
        "id": "FnKBvN82QvvK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 3: backdrop through batchnorm but all in one go\n",
        "# to complete the challenge look at the mathmatical expression\n",
        "# of hte output of the batchnorm, the the derivative w.r.t. its\n",
        "# input, simplify the expression, and just write it out.\n",
        "\n",
        "# forward pass\n",
        "# hprebn = embcat @ W1 + b1 # pre activation\n",
        "# bnmeani = 1 / n * hprebn.sum(0, keepdim=True)\n",
        "# bndiff = hprebn - bnmeani\n",
        "# bndiff2 = bndiff**2\n",
        "# bnvar = 1 / (n - 1) * bndiff2.sum(0, keepdim=True)\n",
        "# bnvar_inv = (bnvar + 1e-5) ** -0.5\n",
        "# bnraw = bndiff * bnvar_inv\n",
        "# hpreact = bngain * bnraw + bnbias\n",
        "\n",
        "# calculate dhprebn given dhpreact\n",
        "\n",
        "#------Code start--------\n",
        "dhprebn = bngain * bnvar_inv/n * (n*dhpreact - dhpreact.sum(0) - n/(n-1)*bnraw*(dhpreact*bnraw).sum(0))\n",
        "#------Code end----------\n",
        "\n",
        "cmp('hprebn', dhprebn, hprebn)"
      ],
      "metadata": {
        "id": "XGghNcJASTYK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}